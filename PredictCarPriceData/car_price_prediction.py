# -*- coding: utf-8 -*-
"""Car_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IoHhOVi5RlzNBtymtXtlGg6Zip5jcgLZ

# Partie 1 : Prétraitement des données
Dataset link : https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho?select=car+data.csv

## Importation des bibliothèques et du jeu de données
"""

import numpy as np  # Est utilisée pour des opérations mathématiques et des calculs numériques
import pandas as pd # Est utilisée pour la manipulation et l'analyse des données
import matplotlib.pyplot as plt # est utilisée pour tracer et visualiser des graphiques
import seaborn as sns # Est une bibliothèque de visualisation de données basée sur matplotlib
# Seaborn offre une interface de haut niveau pour créer des graphiques statistiques attrayants et informatifs.

"""Importer le jeu de données"""

# monter notre Google Drive dans Google Colab
# Cela permet d'accéder aux fichiers et répertoires présents dans le Google Drive à partir de l'environnement Colab.
from google.colab import drive
drive.mount('/content/drive')

# Chargement du fichier CSV dans un objet DataFrame à l'aide de la bibliothèque pandas.
# pd.read_csv() : Est une fonction fournie par la bibliothèque pandas qui permet de lire un fichier CSV et de le charger dans un objet DataFrame.
dataset = pd.read_csv('/content/drive/MyDrive/PredictCarPriceData/car data.csv')

# La méthode head() est utilisée pour afficher les premières lignes d'un DataFrame.
# Lorsqu'on appelez dataset.head(), on verra affiché les premières lignes du DataFrame (dataset).
dataset.head()
#dataset.tail()

"""## Exploration de données"""

# La méthode describe() est utilisée pour générer des statistiques descriptives sur un DataFrame.
# Lorsqu'on appele dataset.describe(), on obtient un résumé statistique des colonnes numériques du DataFrame dataset.
dataset.describe()
# Cela affichera différentes statistiques pour chaque colonne numérique du DataFrame dataset,
# telles que le nombre total d'observations (count), la moyenne (mean), l'écart-type (std), la valeur minimale (min),
# les quartiles (25%, 50%, 75%), et la valeur maximale (max).
# Notez que describe() ne génère des statistiques que pour les colonnes numériques

# Si on souhaite inclure également des colonnes non numériques,
#dataset.describe(include='all')

# L'attribut columns d'un DataFrame pandas est utilisé pour accéder aux noms des colonnes du DataFrame.
# Lorsqu'on appele dataset.columns, on obtient la liste des noms de colonnes du DataFrame dataset.
dataset.columns

# L'attribut shape d'un DataFrame pandas est utilisé pour obtenir les dimensions du DataFrame,
# c'est-à-dire le nombre de lignes et de colonnes qu'il contient.
# Lorsqu'on appele dataset.shape, on obtient un tuple contenant le nombre de lignes et le nombre de colonnes du DataFrame dataset.
dataset.shape

# La méthode info() est utilisée pour obtenir des informations sur les données présentes dans un DataFrame pandas.
# Lorsqu'on appele dataset.info(), on obtient un résumé des informations sur les colonnes du DataFrame dataset.
dataset.info()

# dataset.select_dtypes(include='object').columns est utilisée pour sélectionner les colonnes d'un DataFrame pandas
# qui ont un type de données "object". Lorsqu'on appele cette expression, on obtient les noms des colonnes correspondantes.
dataset.select_dtypes(include='object').columns

# len(dataset.select_dtypes(include='object').columns) est utilisée pour obtenir le nombre de colonnes dans un DataFrame pandas
# qui ont un type de données "object".
len(dataset.select_dtypes(include='object').columns)

# dataset.select_dtypes(include=['float64','int64']).columns est utilisée pour sélectionner les colonnes d'un DataFrame pandas
# qui ont des types de données "float64" et "int64". Lorsqu'on appele cette expression, on obtient les noms des colonnes correspondantes.
dataset.select_dtypes(include=['float64','int64']).columns

# len(dataset.select_dtypes(include=['float64','int64']).columns) est utilisée pour obtenir le nombre de colonnes dans
# un DataFrame pandas qui ont des types de données "float64" et "int64".
len(dataset.select_dtypes(include=['float64','int64']).columns)
# La méthode select_dtypes() est utilisée pour filtrer les colonnes du DataFrame en fonction des types de données,
# et l'argument include=['float64','int64'] spécifie que seules les colonnes avec des types de données "float64" ou "int64" doivent être incluses.

# Synthèse statistique
dataset.describe()

dataset.columns

"""## Traitement des valeurs manquantes"""

# dataset.isnull().values.any() est utilisée pour vérifier s'il y a des valeurs nulles (manquantes) dans notre DataFrame pandas.
dataset.isnull().values.any()
# Elle renvoie True si des valeurs nulles sont présentes dans le DataFrame dataset, et False dans le cas contraire.

# dataset.isnull().values.sum() est utilisée pour calculer le nombre total de valeurs nulles (manquantes) dans un DataFrame pandas.
dataset.isnull().values.sum()

"""## Restructuration de notre ensemble des données"""

# On affiche les premières lignes d'un ensemble de données.
# Cela permet de visualiser rapidement les premières observations du jeu de données afin de se faire une idée de sa structure et de son contenu.
dataset.head()

# suppression la colonne 'Car_Name' du DataFrame dataset.
dataset = dataset.drop(columns='Car_Name')

# On affiche les premières lignes d'un ensemble de données.
dataset.head()

# dataset['Current Year'] = 2023 ajoute une nouvelle colonne appelée "Current Year" au jeu de données dataset
# et lui attribue la valeur 2023 pour toutes les lignes de cette colonne.
dataset['Current Year'] = 2023

# On affiche les premières lignes d'un ensemble de données.
dataset.head()

# Ajout d'une nouvelle colonne appelée "Years Old" à notre ensemble de données, en calculant la différence entre la colonne "Current Year" et la colonne "Year".
dataset['Years Old'] = dataset['Current Year'] - dataset['Year']

# On affiche les premières lignes d'un ensemble de données.
dataset.head()

# Suppression des colonnes Current Year & Year
dataset = dataset.drop(columns=['Current Year','Year'])

# On affiche les premières lignes d'un ensemble de données.
dataset.head()



"""## Encodage des données catégorielles"""

#dataset.select_dtypes(include='object').columns renvoie les noms des colonnes dans le dataframe dataset
# qui ont un type de données "object", c'est-à-dire des colonnes contenant des valeurs de type chaîne de caractères.
dataset.select_dtypes(include='object').columns

# len(dataset.select_dtypes(include='object').columns) permet de compter le nombre de colonnes dans un DataFrame dataset qui ont un type de données "object".
len(dataset.select_dtypes(include='object').columns)

# extraction de la colonne "Fuel_Type" du dataset et calcule du nombre de valeurs uniques présentes dans cette colonne.
# En d'autres termes, on compte le nombre de catégories distinctes de types de carburant présentes dans le dataset.
dataset['Fuel_Type'].nunique()

# dataset['Seller_Type'].nunique() renvoie le nombre de valeurs uniques dans la colonne 'Seller_Type' du dataset.
# Plus précisément, il compte le nombre distinct de catégories de vendeurs dans la colonne 'Seller_Type'.
# Par exemple, si la colonne 'Seller_Type' contient les valeurs "Particulier", "Concessionnaire" et "Autre", alors la fonction "nunique()" renverra le nombre 3,
# indiquant qu'il y a 3 catégories de vendeurs différentes dans cette colonne.
dataset['Seller_Type'].nunique()

# extraction de la colonne "Transmission" du jeu de données et comptage le nombre de valeurs uniques dans cette colonne.
dataset['Transmission'].nunique()

# L'attribut shape d'un DataFrame pandas est utilisé pour obtenir les dimensions du DataFrame,
# c'est-à-dire le nombre de lignes et de colonnes qu'il contient.
dataset.shape

# dataset = pd.get_dummies(data=dataset, drop_first=True) effectue une opération de codage one-hot sur les variables catégorielles du DataFrame dataset.
# pd.get_dummies() de la bibliothèque Pandas est utilisée pour effectuer cette opération de codage.
# Elle convertit chaque variable catégorielle en une série de variables binaires, également appelées variables indicatrices.
# Ces variables binaires indiquent la présence ou l'absence d'une catégorie spécifique dans la variable d'origine.
dataset = pd.get_dummies(data=dataset, drop_first=True)

# On affiche les premières lignes d'un ensemble de données.
dataset.head()

# L'attribut shape d'un DataFrame pandas est utilisé pour obtenir les dimensions du DataFrame,
# c'est-à-dire le nombre de lignes et de colonnes qu'il contient.
dataset.shape

"""## Matrice de corrélation"""

# supprission de la colonne "Selling_Price" du DataFrame "dataset" et assignation du DataFrame modifié à la variable "dataset_2".
dataset_2 = dataset.drop(columns='Selling_Price')

# génèration d'un graphique à barres qui représente les corrélations entre la colonne "Selling_Price" du dataset dataset
# et les autres colonnes du dataset_2. Chaque barre du graphique représente le niveau de corrélation entre la colonne "Selling_Price" et une autre colonne du dataset.
dataset_2.corrwith(dataset['Selling_Price']).plot.bar(
    figsize=(16,9), title='Correlated with Selling Price', grid=True
)
# Corrélation positive(barre est orientée vers le haut) : Cela indique une corrélation positive entre la colonne "Selling_Price" et la variable correspondante.
# Cela signifie que lorsque la valeur de la variable augmente, le prix de vente tend également à augmenter.
# Une corrélation proche de 1 indique une forte corrélation positive, tandis qu'une corrélation proche de 0 indique une corrélation faible.
#--------------------------------------------------
# Corrélation négative(barre est orientée vers le bas) : Cela indique une corrélation négative entre la colonne "Selling_Price" et la variable correspondante.
# Cela signifie que lorsque la valeur de la variable augmente, le prix de vente tend à diminuer.
# Une corrélation proche de -1 indique une forte corrélation négative, tandis qu'une corrélation proche de 0 indique une corrélation faible.
#--------------------------------------------------
# Corrélation faible(barres proches de zéro) : Indiquent une corrélation faible ou inexistante entre la colonne "Selling_Price" et la variable correspondante.
# Cela signifie que la variable n'a pas d'impact significatif sur le prix de vente.
#--------------------------------------------------
# Corrélation nulle(barre est absente:hauteur de la barre égale à zéro) : Cela signifie qu'il n'y a aucune corrélation linéaire entre la colonne "Selling_Price" et la variable correspondante.
# Cela ne signifie pas nécessairement qu'il n'y a pas de relation entre les variables, mais simplement qu'il n'y a pas de corrélation linéaire.

# Calcule la corrélation entre les différentes variables de notre ensemble de données.
corr = dataset.corr()
# La sortie du code sera une nouvelle DataFrame appelée "corr",
# où chaque valeur représente le coefficient de corrélation entre deux variables.
# Ce coefficient de corrélation mesure la force et la direction de la relation linéaire entre deux variables.
# Il varie de -1 à 1, où -1 indique une corrélation négative parfaite, 1 indique une corrélation positive parfaite, et 0 indique l'absence de corrélation linéaire.

# Heatmap
# Les dimensions de la figure sont spécifiées avec plt.figure(figsize=(16,9)),
# indiquant que la largeur de la figure sera de 16 pouces et la hauteur de 9 pouces.
plt.figure(figsize=(16,9))
sns.heatmap(corr, annot=True)
# La fonction sns.heatmap() est utilisée pour tracer la Heatmap.
# Elle prend en entrée une matrice de données corrélées (corr) et affiche des couleurs différentes pour représenter les valeurs de corrélation entre les variables.
# L'option annot=True permet d'afficher les valeurs de corrélation sur la carte Heatmap.

"""## Fractionnement de l'ensemble de données"""

# On affiche les premières lignes d'un ensemble de données.
dataset.head()

# Matrice des caractéristiques
# suppression de la colonne "Selling_Price" de l'ensemble de notre données et stockage du résultat dans une nouvelle variable appelée "x".
x = dataset.drop(columns='Selling_Price')

# Variable cible
# On définie y comme étant égale à dataset['Selling_Price'], ce qui suggère que la colonne "Selling_Price" est extraite du jeu de données et assignée à la variable y.
y = dataset['Selling_Price']

# utilisation de la fonction train_test_split du module sklearn.model_selection de la bibliothèque scikit-learn
# pour diviser un ensemble de données en un ensemble d'entraînement et un ensemble de test.
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
# train_test_split prend en entrée deux tableaux x et y qui représentent respectivement
# les caractéristiques (features) et les étiquettes (labels) des données à diviser.
# La fonction train_test_split divise les données en utilisant le paramètre test_size pour spécifier la proportion
# d'échantillons qui seront assignés à l'ensemble de test. Dans cet exemple, test_size=0.2 indique que 20% des données
# seront utilisées pour l'ensemble de test, tandis que les 80% restants seront utilisés pour l'ensemble d'entraînement.

# x_train.shape permet de connaître le nombre de lignes et de colonnes présentes dans le tableau "x_train".
# Le résultat retourné est une séquence de deux nombres, dans le format (nombre_de_lignes, nombre_de_colonnes).
x_train.shape

# y_train représenter les étiquettes ou les sorties attendues d'un modèle.
y_train.shape
# La propriété "shape" retourne les dimensions de l'objet.
# Pour un tableau ou une matrice, cela donne le nombre de lignes et de colonnes.

# x_test représenter les données de test.
x_test.shape
# (shape) renvoie les dimensions du tableau ou de la matrice, c'est-à-dire le nombre de lignes et de colonnes.
# Il retourne aussi un tuple de la forme (nombre de lignes, nombre de colonnes).

# y_test contient les valeurs cibles d'un ensemble de données de test.
y_test.shape
# (shape) renvoie les dimensions du tableau ou de la matrice, c'est-à-dire le nombre de lignes et de colonnes.
# NB : est un tableau unidimensionnel de taille 61

"""## Mise à l'échelle des fonctionnalités

# Part 2 : Construction du modèle

## Régression linéaire multiple
"""

# from sklearn.linear_model import LinearRegression : Cette ligne importe la classe LinearRegression du module linear_model de la bibliothèque scikit-learn
# Cela permet d'utiliser le modèle de régression linéaire pour effectuer des prédictions.
from sklearn.linear_model import LinearRegression
# création d'une instance du modèle de régression linéaire en utilisant la classe LinearRegression().
# L'instance du modèle est stockée dans la variable regressor_mlr.
regressor_mlr = LinearRegression()
# Entraînement du modèle de régression linéaire sur les données d'entraînement. Les variables x_train et y_train contiennent respectivement
# les caractéristiques (ou les variables indépendantes) et les étiquettes (ou les variables dépendantes) des données d'entraînement.
# L'appel à la méthode fit() ajuste le modèle aux données d'entraînement, ce qui permet de trouver les coefficients optimaux pour la régression linéaire.
regressor_mlr.fit(x_train, y_train)

# utilisation du modèle de régression linéaire (regressor_mlr) entraîné précédemment pour effectuer des prédictions
# sur les données de test (x_test). Les prédictions résultantes sont stockées dans la variable y_pred.
y_pred = regressor_mlr.predict(x_test)

# importe la fonction r2_score du module metrics de scikit-learn.
from sklearn.metrics import r2_score

# appelle la fonction r2_score avec deux arguments, y_test et y_pred.
# La fonction r2_score prend ces deux tableaux de données en entrée et calcule
# le coefficient de détermination (R²) pour évaluer les performances du modèle de régression linéaire.
r2_score(y_test, y_pred)

"""## Random forest regression"""

# Importe la classe RandomForestRegressor du module ensemble de la bibliothèque scikit-learn.
from sklearn.ensemble import RandomForestRegressor
# création d'une instance du modèle RandomForestRegressor en utilisant la classe RandomForestRegressor()
regressor_rf = RandomForestRegressor()
# entraînement du modèle RandomForestRegressor sur les données d'entraînement.
# Les variables x_train et y_train contiennent respectivement les caractéristiques (ou les variables indépendantes)
# et les étiquettes (ou les variables dépendantes) des données d'entraînement.
# L'appel à la méthode fit() ajuste le modèle aux données d'entraînement,
# ce qui permet de trouver les paramètres optimaux pour la régression par forêts aléatoires.
regressor_rf.fit(x_train, y_train)

# utilisation du modèle de régression par Random forest regression (regressor_rf) entraîné précédemment
# pour effectuer des prédictions sur les données de test (x_test). Les prédictions résultantes sont stockées dans la variable y_pred.
y_pred = regressor_rf.predict(x_test)

# importe la fonction r2_score du module metrics de scikit-learn.
from sklearn.metrics import r2_score

# appelle la fonction r2_score avec deux arguments, y_test et y_pred.
# La fonction r2_score prend ces deux tableaux de données en entrée et calcule
# le coefficient de détermination (R²) pour évaluer les performances du modèle de régression linéaire.
r2_score(y_test, y_pred)

"""R² avec la méthode de régression linéaire multiple : 0.9144342972228519
Cela indique que le modèle de régression linéaire multiple explique environ 91,4% de la variance des données. Il y a une bonne correspondance entre les valeurs prédites et les valeurs réelles, mais il reste une certaine variance non expliquée par le modèle.
R² avec la méthode de régression par forêts aléatoires : 0.9610282298948004
Cela indique que le modèle de régression par forêts aléatoires explique environ 96,1% de la variance des données. Il y a une correspondance plus élevée entre les valeurs prédites et les valeurs réelles, avec une plus faible variance résiduelle par rapport à la méthode de régression linéaire multiple.

On peut en déduire que la méthode de régression par Random forest regression offre une meilleure performance de prédiction par rapport à la méthode de régression linéaire multiple dans ce cas spécifique.

# Partie 3 : Trouver les paramètres optimaux à l'aide de RandomizedSearchCV
"""

# importe la classe RandomizedSearchCV du module model_selection de la bibliothèque scikit-learn (sklearn).
from sklearn.model_selection import RandomizedSearchCV

# définit un dictionnaire parameters contenant différentes valeurs pour les hyperparamètres d'un modèle.
# n_estimators : Il s'agit du nombre d'arbres dans le modèle RandomForestRegressor (ou un autre modèle utilisant cet hyperparamètre).
# criterion : Il s'agit de la fonction de critère utilisée pour mesurer la qualité d'une scission dans le modèle.
# max_depth : Il s'agit de la profondeur maximale des arbres dans le modèle.
# min_samples_split : Il s'agit du nombre minimum d'échantillons requis pour effectuer une scission à un nœud interne de l'arbre.
# min_samples_leaf : Il s'agit du nombre minimum d'échantillons requis pour être à un nœud feuille de l'arbre.
# max_features : Il s'agit du nombre de caractéristiques à considérer lors de la recherche de la meilleure scission.
parameters = {
    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],
    'criterion': ['squared_error', 'absolute_error'],
    'max_depth': [10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10, 20, 50],
    'min_samples_leaf': [1, 2, 5, 10],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Affichage parameters
parameters

# RandomizedSearchCV du module sklearn.model_selection, permet d'effectuer une recherche aléatoire (randomized search)
# sur un espace d'hyperparamètres donné pour trouver les meilleures combinaisons d'hyperparamètres pour un modèle d'apprentissage automatique.
# estimator: L'estimateur de modèle à utiliser lors de la recherche des hyperparamètres optimaux.
# param_distributions: Un dictionnaire des distributions des hyperparamètres à explorer.
# Chaque clé du dictionnaire représente un hyperparamètre, et la valeur associée est une distribution de valeurs possibles pour cet hyperparamètre.
# n_iter: Le nombre d'itérations de la recherche aléatoire à effectuer.
# Dans votre cas, vous avez spécifié n_iter=10, ce qui signifie que 10 combinaisons d'hyperparamètres seront évaluées.
# scoring: La métrique de score à utiliser pour évaluer les performances des différentes combinaisons d'hyperparamètres.
# neg_mean_absolute_error', qui signifie l'erreur absolue moyenne négative.
# Cette métrique mesure l'écart moyen entre les valeurs prédites et les valeurs réelles de notre variable cible,
# mais avec une valeur négative car la recherche du modèle optimal cherche à minimiser cette métrique.
# cv: Le nombre de plis (folds) à utiliser lors de la validation croisée.
# verbose: Le niveau de verbosité pour afficher les messages de progression pendant la recherche.
# n_jobs: Le nombre de tâches en parallèle à exécuter pendant la recherche.
random_cv = RandomizedSearchCV(estimator=regressor_rf, param_distributions=parameters, n_iter=10,
                               scoring='neg_mean_absolute_error', cv=5, verbose=2, n_jobs=-1)

# random_cv.fit(x_train, y_train) lance le processus de recherche aléatoire des meilleures
# combinaisons d'hyperparamètres pour votre modèle de régression à l'aide de la méthode fit()
# de l'objet random_cv, qui est une instance de la classe RandomizedSearchCV.
random_cv.fit(x_train, y_train)

# random_cv.best_estimator_ renvoie le meilleur estimateur (modèle) trouvé lors de la recherche aléatoire des hyperparamètres.
random_cv.best_estimator_

"""Cela signifie que le meilleur estimateur est un modèle de régression RandomForestRegressor avec les hyperparamètres suivants :

criterion='absolute_error' : Le critère de fractionnement utilisé est l'erreur absolue.
max_depth=10 : La profondeur maximale de chaque arbre de décision est limitée à 10.
max_features='auto' : Le nombre maximal de caractéristiques à considérer lors de la recherche de la meilleure fraction de scission est déterminé automatiquement en fonction de la taille de l'ensemble de données.
n_estimators=300 : Le nombre d'estimateurs (arbres de décision) dans Random forest regression est fixé à 300.
"""

# random_cv.best_params_ renvoie un dictionnaire contenant les meilleurs paramètres (hyperparamètres) trouvés lors de la recherche aléatoire des hyperparamètres.
random_cv.best_params_

"""Le résultat que nous avons obtenu indique les meilleurs paramètres identifiés lors de la recherche aléatoire des hyperparamètres pour notre modèle RandomForestRegressor.

n_estimators: La meilleure valeur est 300, ce qui signifie qu'il y a 300 arbres dans la Random forest.
min_samples_split: La meilleure valeur est 2, ce qui indique le nombre minimum d'échantillons requis pour effectuer une division supplémentaire d'un nœud de l'arbre.
min_samples_leaf: La meilleure valeur est 1, ce qui indique le nombre minimum d'échantillons requis pour former une feuille d'un arbre.
max_features: La valeur 'auto' indique que le nombre maximal de caractéristiques à considérer lors de la recherche de la meilleure fraction de scission est déterminé automatiquement en fonction de la taille de l'ensemble de données.
max_depth: La meilleure profondeur maximale est 10, ce qui limite la profondeur de chaque arbre de décision à 10.
criterion: Le meilleur critère de fractionnement est 'absolute_error', ce qui signifie que l'erreur absolue est utilisée comme critère de fractionnement lors de la construction des arbres.

## Modèle final (Random forest)
"""

from sklearn.ensemble import RandomForestRegressor
#regressor = RandomForestRegressor(max_depth=10, max_features='log2', n_estimators=600)
# regressor = RandomForestRegressor() crée une instance d'un modèle de régression RandomForestRegressor.
regressor = RandomForestRegressor()
# regressor.fit(x_train, y_train), entraîne le modèle RandomForestRegressor avec les données d'entraînement x_train et les valeurs cibles y_train.
regressor.fit(x_train, y_train)

# y_pred = regressor.predict(x_test) effectue des prédictions sur les données de test x_test à l'aide du modèle RandomForestRegressor entraîné précédemment.
y_pred = regressor.predict(x_test)

from sklearn.metrics import r2_score
# r2_score(y_test, y_pred) calcule le coefficient de détermination R2 (R-squared) pour évaluer la performance du modèle de régression sur les données de test.
# prend deux arguments : les valeurs cibles réelles (y_test) et les valeurs prédites par le modèle (y_pred).
# Elle compare les deux en utilisant la formule du coefficient de détermination R2 et retourne le score R2.
r2_score(y_test, y_pred)
# Le coefficient de détermination R2 mesure la proportion de la variance des valeurs cibles (y_test)
# expliquée par les prédictions du modèle (y_pred). Il varie de 0 à 1, où une valeur de 1 indique
# une prédiction parfaite, et une valeur proche de 0 indique que le modèle n'explique pas bien les variations des données.

"""La valeur de 0.9521311191835508 pour r2_score(y_test, y_pred) signifie que le modèle RandomForestRegressor explique environ 95.21% de la variance des valeurs cibles (y_test) à l'aide de ses prédictions (y_pred) sur les données de test.

En d'autres termes, cela indique que le modèle a une bonne adéquation aux données de test, car il parvient à capturer une grande partie des variations présentes dans les valeurs cibles. Plus le score R2 est proche de 1, meilleure est l'adéquation du modèle aux données.

## Prédiction d'une seule observation
"""

# On affiche les premières lignes d'un ensemble de données.
dataset.head()

# feature_names est une liste qui contient les noms des caractéristiques (features) utilisées dans notre modèle.
# single_obs est une liste contenant les valeurs d'une seule observation (exemple) pour chaque caractéristique.
# Ces valeurs correspondent à une voiture spécifique que vous souhaitez prédire.
# df_single_obs est créé en utilisant pd.DataFrame(single_obs, columns=feature_names).
# Il s'agit d'un DataFrame Pandas contenant une seule ligne de données (single_obs) avec les noms des colonnes correspondants (feature_names).
# prediction est une variable qui contient les prédictions faites par le modèle RandomForestRegressor pour l'observation unique donnée (df_single_obs).
# Ces prédictions sont obtenues en appelant la méthode predict() du modèle sur df_single_obs.
feature_names = ['Present_Price', 'Kms_Driven', 'Owner', 'Years Old', 'Fuel_Type_Diesel', 'Fuel_Type_Petrol', 'Seller_Type_Individual', 'Transmission_Manual']
single_obs = [[8.54, 3500, 0, 5, 1, 0, 0, 1]]

df_single_obs = pd.DataFrame(single_obs, columns=feature_names)
prediction = regressor.predict(df_single_obs)

# Affichage de prediction
prediction
